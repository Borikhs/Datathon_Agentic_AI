"""
Chainlit UI for MCP Multi-Server Client
data_client.pyì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ì›¹ UIë¡œ ì œê³µí•©ë‹ˆë‹¤.
"""
import json
import chainlit as cl
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

model = ChatOpenAI(model="gpt-4o", streaming=True)
user_id = "khs"


def load_mcp_config():
    """MCP ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open("./mcp_config.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None


def create_server_config():
    """ì„œë²„ ì„¤ì • ìƒì„±"""
    config = load_mcp_config()
    server_config = {}

    if config and "mcpServers" in config:
        for server_name, server_data in config["mcpServers"].items():
            if "command" in server_data:
                server_config[server_name] = {
                    "command": server_data.get("command"),
                    "args": server_data.get("args", []),
                    "transport": "stdio",
                }
    return server_config


@cl.on_chat_start
async def start():
    """ì±„íŒ… ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    await cl.Message(content="ğŸ¤– MCP ì„œë²„ ì—°ê²° ì¤‘...").send()

    try:
        # MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        server_config = create_server_config()

        if not server_config:
            await cl.Message(content="âŒ MCP ì„œë²„ ì„¤ì •ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.").send()
            return

        client = MultiServerMCPClient(server_config)
        tools = await client.get_tools()

        server_names = list(server_config.keys())

        # ì„¸ì…˜ì— ì €ì¥
        cl.user_session.set("client", client)
        cl.user_session.set("tools", tools)
        cl.user_session.set("chat_history", [])

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = SystemMessage(
            content=(
                f"ë˜ë„ë¡ì´ë©´ ëª¨ë“  ìš”ì²­ì— ìˆœì°¨ì  ì‚¬ê³ (Sequentialthinking) ë„êµ¬ë¥¼ ì´ìš©í•´ í•œêµ­ì–´ë¡œ ì‚¬ê³ í•˜ì„¸ìš”. "
                "Windows-Command-Line-MCP-Server, mem0-mcp, MCP SQLite Server, desktop-commander, pubmed-mcp-serverì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”. "
                f"mem0-memory íˆ´ì„ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìì˜ ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ë©´ì„œ ëŒ€í™”í•˜ì„¸ìš”. "
                f"ì œ UserIDëŠ” {user_id} ì…ë‹ˆë‹¤. "
            )
        )
        cl.user_session.set("system_prompt", system_prompt)

        # ì„±ê³µ ë©”ì‹œì§€
        success_msg = f"""
âœ… **MCP ì„œë²„ ì—°ê²° ì™„ë£Œ!**

ğŸ“Š **ë¡œë“œëœ íˆ´**: {len(tools)}ê°œ
ğŸ”— **ì—°ê²°ëœ ì„œë²„**: {', '.join(server_names)}

ğŸ’¡ ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!
        """
        await cl.Message(content=success_msg).send()

    except Exception as e:
        error_msg = f"âŒ **ì´ˆê¸°í™” ì˜¤ë¥˜**: {str(e)}"
        await cl.Message(content=error_msg).send()
        import traceback
        traceback.print_exc()


@cl.on_message
async def main(message: cl.Message):
    """ë©”ì‹œì§€ ì²˜ë¦¬"""
    # ì„¸ì…˜ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    tools = cl.user_session.get("tools")
    chat_history = cl.user_session.get("chat_history")
    system_prompt = cl.user_session.get("system_prompt")

    if not tools:
        await cl.Message(content="âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.").send()
        return

    # ì—ì´ì „íŠ¸ ìƒì„± (ë§¤ë²ˆ ìƒì„±)
    from langgraph.prebuilt import create_react_agent
    agent = create_react_agent(model, tools)

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    chat_history.append(HumanMessage(content=message.content))

    # ë§¤ ì¿¼ë¦¬ë§ˆë‹¤ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    messages_with_system = [system_prompt] + chat_history

    # ì‘ë‹µ ë©”ì‹œì§€ ìƒì„±
    msg = cl.Message(content="")
    await msg.send()

    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        final_messages = None
        tool_calls_text = ""

        async for event in agent.astream_events(
            {"messages": messages_with_system},
            version="v2",
            config={"recursion_limit": 100}
        ):
            kind = event["event"]

            # íˆ´ í˜¸ì¶œ í‘œì‹œ
            if kind == "on_tool_start":
                tool_name = event.get("name", "unknown")
                tool_input = event.get("data", {}).get("input", {})
                tool_calls_text += f"\n\nğŸ”§ **[TOOL CALL]** `{tool_name}`\n```json\n{json.dumps(tool_input, indent=2, ensure_ascii=False)}\n```"
                await msg.stream_token(tool_calls_text)
                tool_calls_text = ""

            # íˆ´ ê²°ê³¼ í‘œì‹œ
            elif kind == "on_tool_end":
                tool_output = event.get("data", {}).get("output", "")

                # ToolMessage ê°ì²´ì¸ ê²½ìš° contentë§Œ ì¶”ì¶œ
                if hasattr(tool_output, 'content'):
                    output_str = tool_output.content
                else:
                    output_str = str(tool_output) if tool_output else ""

                # ê¸€ììˆ˜ ì œí•œ ì—†ì´ ì „ì²´ ì¶œë ¥
                tool_calls_text += f"\nâœ… **[TOOL RESULT]**\n```\n{output_str}\n```\n"
                await msg.stream_token(tool_calls_text)
                tool_calls_text = ""

            # LLM ìŠ¤íŠ¸ë¦¬ë°
            elif kind == "on_chat_model_stream":
                content = event.get("data", {}).get("chunk", {})
                if hasattr(content, "content") and content.content:
                    await msg.stream_token(content.content)

            # ìµœì¢… ê²°ê³¼
            elif kind == "on_chain_end":
                output = event.get("data", {}).get("output")
                if output:
                    if isinstance(output, dict) and "messages" in output:
                        final_messages = output["messages"]
                    elif isinstance(output, list):
                        final_messages = output

        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if final_messages:
            cl.user_session.set("chat_history", final_messages)

        await msg.update()

    except Exception as e:
        error_msg = f"\n\nâŒ **ì—ëŸ¬ ë°œìƒ**: {str(e)}"
        await msg.stream_token(error_msg)
        await msg.update()
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    from chainlit.cli import run_chainlit
    run_chainlit(__file__)
